{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import robot_data_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        Initializes a DataGenerator object.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file containing the training data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "\n",
    "        # Read in data from file and convert to lowercase\n",
    "        with open(path) as f:\n",
    "            data = f.read().lower()\n",
    "\n",
    "        # Create list of unique characters in the data\n",
    "        self.chars = list(set(data))\n",
    "\n",
    "        # Create dictionaries mapping characters to and from their index in the list of unique characters\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(self.chars)}\n",
    "\n",
    "        # Set the size of the vocabulary (i.e. number of unique characters)\n",
    "        self.vocab_size = len(self.chars)\n",
    "\n",
    "        # Read in examples from file and convert to lowercase, removing leading/trailing white space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "        self.examples = [x.lower().strip() for x in examples]\n",
    "\n",
    "    def generate_example(self, idx):\n",
    "        \"\"\"\n",
    "        Generates an input/output example for the language model based on the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the example to generate.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the input and output arrays for the example.\n",
    "        \"\"\"\n",
    "        example_chars = self.examples[idx]\n",
    "\n",
    "        # Convert the characters in the example to their corresponding indices in the list of unique characters\n",
    "        example_char_idx = [self.char_to_idx[char] for char in example_chars]\n",
    "\n",
    "        # Add newline character as the first character in the input array, and as the last character in the output array\n",
    "        X = [self.char_to_idx['\\n']] + example_char_idx\n",
    "        Y = example_char_idx + [self.char_to_idx['\\n']]\n",
    "\n",
    "        return np.array(X), np.array(Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_robot:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (RNN).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the RNN.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the RNN.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the RNN.\n",
    "    learning_rate : float\n",
    "        The learning rate used during training.\n",
    "    is_initialized : bool\n",
    "        Indicates whether the AdamW parameters has been initialized.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, learning_rate)\n",
    "        Initializes an instance of the RNN class.\n",
    "\n",
    "    forward(self, X, a_prev)\n",
    "     Computes the forward pass of the RNN.\n",
    "\n",
    "    softmax(self, x)\n",
    "       Computes the softmax activation function for a given input array.\n",
    "\n",
    "    backward(self,x, a, y_preds, targets)\n",
    "        Implements the backward pass of the RNN.\n",
    "\n",
    "   loss(self, y_preds, targets)\n",
    "     Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "    adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4)\n",
    "       Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "\n",
    "    train(self, generated_names=5)\n",
    "       Trains the RNN on a dataset using backpropagation through time (BPTT).\n",
    "\n",
    "   predict(self, start)\n",
    "        Generates a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainSet, valSet, hidden_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the RNN class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the RNN.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the RNN.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the RNN.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.trainSet = trainSet\n",
    "        self.valSet = valSet\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.X = None\n",
    "        self.output_size = 3\n",
    "        self.input_size = 5\n",
    "\n",
    "        # model parameters\n",
    "        self.Wax = np.random.uniform(-np.sqrt(1. / self.input_size), np.sqrt(1. / self.input_size), (hidden_size, self.input_size))\n",
    "        self.Waa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (hidden_size, hidden_size))\n",
    "        self.Wya = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size), (self.output_size, hidden_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.dWax, self.dWaa, self.dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        self.dba, self.dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "\n",
    "        # parameter update with AdamW\n",
    "        self.mWax = np.zeros_like(self.Wax)\n",
    "        self.vWax = np.zeros_like(self.Wax)\n",
    "        self.mWaa = np.zeros_like(self.Waa)\n",
    "        self.vWaa = np.zeros_like(self.Waa)\n",
    "        self.mWya = np.zeros_like(self.Wya)\n",
    "        self.vWya = np.zeros_like(self.Wya)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, a_prev):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the RNN.\n",
    "\n",
    "        Parameters:\n",
    "        X (ndarray): Input data of shape (seq_length, input_size)\n",
    "        a_prev (ndarray): Activation of the previous time step of shape (hidden_size, 1)\n",
    "\n",
    "        Returns:\n",
    "        x (dict): Dictionary of input data of shape (seq_length, input_size, 1), with keys from 0 to seq_length-1\n",
    "        a (dict): Dictionary of hidden activations for each time step, with keys from 0 to seq_length-1\n",
    "        y_pred (dict): Dictionary of output global positions for each time step, with keys from 0 to seq_length-1\n",
    "        \"\"\"\n",
    "        # Initialize dictionaries to store activations and output probabilities.\n",
    "        x, a, y_pred = {}, {}, {}\n",
    "\n",
    "        # Store the input data in the class variable for later use in the backward pass.\n",
    "        self.X = X\n",
    "        #print(\"input: \", self.X)\n",
    "        # Set the initial activation to the previous activation.\n",
    "        a[-1] = np.copy(a_prev)\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(len(self.X)):\n",
    "\n",
    "            # get the input at the current time step\n",
    "            x[t] = np.zeros((self.input_size,1))\n",
    "            #if (self.X[t] != None):\n",
    "            #    x[t][self.X[t]] = 1\n",
    "            x[t] = np.array(X[t])\n",
    "            x[t] = x[t].reshape((self.input_size,1))\n",
    "            # compute the hidden activation at the current time step\n",
    "#            print(f\"{self.Wax.shape=}\")\n",
    "#            print(f\"{x[t].shape=}\")\n",
    "#            print(f\"{self.Waa.shape=}\")\n",
    "#            print(f\"{a[t - 1].shape=}\")\n",
    "#            print(f\"{self.ba.shape=}\")\n",
    "#            print(\"\\n\\n\")\n",
    "            a[t] = np.tanh(np.dot(self.Wax, x[t]) + np.dot(self.Waa, a[t - 1]) + self.ba)\n",
    "            #print(\"a[t] shape:\", a[t].shape)\n",
    "            # compute the output probabilities at the current time step\n",
    "            #print(\"self.Wya shape \", self.Wya.shape)\n",
    "#            print(f\"{a[t].shape=}\")\n",
    "#            print(f\"{self.by.shape=}\")\n",
    "#            print(f\"{self.Wya.shape=}\")\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wya, a[t]) + self.by)\n",
    "#            print(f\"{y_pred[t].shape=}\")\n",
    "            #print(\"y[t] shape \", y_pred[t].shape)\n",
    "            # add an extra dimension to X to make it compatible with the shape of the input to the backward pass\n",
    "        # return the input, hidden activations, and output probabilities at each time step\n",
    " #       print(f\"{y_pred[0].shape=}\")\n",
    "        return x, a, y_pred\n",
    "\n",
    "    def backward(self,x, a, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Implement the backward pass of the RNN.\n",
    "\n",
    "        Args:\n",
    "        x -- (dict) of input characters (as one-hot encoding vectors) for each time-step, shape (vocab_size, sequence_length)\n",
    "        a -- (dict) of hidden state vectors for each time-step, shape (hidden_size, sequence_length)\n",
    "        y_preds -- (dict) of output probability vectors (after softmax) for each time-step, shape (vocab_size, sequence_length)\n",
    "        targets -- (list) of integer target characters (indices of characters in the vocabulary) for each time-step, shape (1, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize derivative of hidden state for the last time-step\n",
    "        da_next = np.zeros_like(a[0])\n",
    "\n",
    "        # Loop through the input sequence backwards\n",
    "        for t in reversed(range(len(self.X))):\n",
    "            #print(t)\n",
    "            #FIXME: FIX THIS DERIVATIVE\n",
    "            #dy_preds[targets[t]](1/(len(self.X)-t))*np.square(y_preds[i] - targets[i])\n",
    "            # Calculate derivative of output probability vector\n",
    "            dy_preds = np.copy(y_preds[t])\n",
    "            for i in range(len(dy_preds)):\n",
    "                #print(f\"{y_preds[i]=}\")\n",
    "                #print(f\"{targets[i]=}\")\n",
    "                dy_preds[i] = (1/(len(self.X)-t))*np.square(y_preds[i][0] - targets[t][i])\n",
    "            #print(dy_preds)\n",
    "            #print(len(dy_preds))\n",
    "            #print(len(y_preds))\n",
    "            #print(targets[t])\n",
    "            #print(\"all targets\")\n",
    "            #print(targets)\n",
    "            #for ()\n",
    "\n",
    "            for index in range(len(dy_preds)):\n",
    "                dy_preds[index] = (2.0/(self.sequence_length-t))*(dy_preds[index] - targets[t][index])\n",
    "\n",
    "            #dy_preds[targets[t]] -= 1\n",
    "\n",
    "            # Calculate derivative of hidden state\n",
    "            da = np.dot(self.Waa.T, da_next) + np.dot(self.Wya.T, dy_preds)\n",
    "#            print(self.Waa.T.shape, da_next.shape)\n",
    "#            print(self.Wya.T.shape, dy_preds.shape)\n",
    "#            print(\"bbb\")\n",
    "            dtanh = (1 - np.power(a[t], 2))\n",
    "#            print(self.dba.shape)\n",
    "            da_unactivated = dtanh * da\n",
    "\n",
    "            # Calculate gradients\n",
    "            self.dba += da_unactivated\n",
    "            self.dWax += np.dot(da_unactivated, x[t].T)\n",
    "            self.dWaa += np.dot(da_unactivated, a[t - 1].T)\n",
    "\n",
    "            # Update derivative of hidden state for the next iteration\n",
    "            da_next = da_unactivated\n",
    "\n",
    "            # Calculate gradient for output weight matrix\n",
    "            self.dWya += np.dot(dy_preds, a[t].T)\n",
    "\n",
    "            # clip gradients to avoid exploding gradients\n",
    "            for grad in [self.dWax, self.dWaa, self.dWya, self.dba, self.dby]:\n",
    "                np.clip(grad, -1, 1, out=grad)\n",
    "\n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, 1) containing the predicted velocities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "\n",
    "        sum = 0\n",
    "        for i in range(len(y_preds)):\n",
    "            diff = y_preds[i] - targets[i].reshape((3,1))\n",
    "            #print(\"aaaaaaaa \", diff)\n",
    "            prod = np.dot(diff, diff.T)\n",
    "            sum += np.sum((1/(len(y_preds)-i))*prod)\n",
    "        #print(\"\\nsum \", sum)\n",
    "        return sum\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the RNN's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        # AdamW update for Wax\n",
    "        self.mWax = beta1 * self.mWax + (1 - beta1) * self.dWax\n",
    "        self.vWax = beta2 * self.vWax + (1 - beta2) * np.square(self.dWax)\n",
    "        m_hat = self.mWax / (1 - beta1)\n",
    "        v_hat = self.vWax / (1 - beta2)\n",
    "        self.Wax -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wax)\n",
    "\n",
    "        # AdamW update for Waa\n",
    "        self.mWaa = beta1 * self.mWaa + (1 - beta1) * self.dWaa\n",
    "        self.vWaa = beta2 * self.vWaa + (1 - beta2) * np.square(self.dWaa)\n",
    "        m_hat = self.mWaa / (1 - beta1)\n",
    "        v_hat = self.vWaa / (1 - beta2)\n",
    "        self.Waa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Waa)\n",
    "\n",
    "        # AdamW update for Wya\n",
    "        self.mWya = beta1 * self.mWya + (1 - beta1) * self.dWya\n",
    "        self.vWya = beta2 * self.vWya + (1 - beta2) * np.square(self.dWya)\n",
    "        m_hat = self.mWya / (1 - beta1)\n",
    "        v_hat = self.vWya / (1 - beta2)\n",
    "        self.Wya -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wya)\n",
    "\n",
    "        # AdamW update for ba\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Sample a sequence of characters from the RNN.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "        \"\"\"\n",
    "        # initialize input and hidden state\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # create an empty list to store the generated character indices\n",
    "        indices = []\n",
    "\n",
    "        # idx is a flag to detect a newline character, initialize it to -1\n",
    "        idx = -1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        counter = 0\n",
    "        max_chars = 50 # maximum number of characters to generate\n",
    "        newline_character = self.data_generator.char_to_idx['\\n'] # the newline character\n",
    "\n",
    "        while (idx != newline_character and counter != max_chars):\n",
    "            # compute the hidden state\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(list(range(self.vocab_size)), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # store the sampled character index in the list\n",
    "            indices.append(idx)\n",
    "\n",
    "            # update the previous hidden state\n",
    "            a_prev = a\n",
    "\n",
    "            # increment the counter\n",
    "            counter += 1\n",
    "\n",
    "        # return the list of sampled character indices\n",
    "        return indices\n",
    "\n",
    "\n",
    "    def train(self, generated_names=5):\n",
    "        \"\"\"\n",
    "        Train the RNN on a dataset using backpropagation through time (BPTT).\n",
    "\n",
    "        Args:\n",
    "        - generated_names: an integer indicating how many example names to generate during training.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        iter_num = 0\n",
    "        threshold = 5 # stopping criterion for training\n",
    "        smooth_loss = 100#-np.log(1.0 / self.data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "\n",
    "        self.trainSet\n",
    "        self.valSet\n",
    "\n",
    "        while (iter_num < len(self.trainSet)):\n",
    "            a_prev = np.zeros((self.hidden_size, 1))\n",
    "            #idx = iter_num % self.vocab_size\n",
    "            # get a batch of inputs and targets\n",
    "            #inputs, targets = self.data_generator.generate_example(idx)\n",
    "            inputs = self.trainSet[iter_num][0]\n",
    "            #print(\"aaaaaaaa \", inputs)\n",
    "            targets = self.trainSet[iter_num][1]\n",
    "\n",
    "            # forward pass\n",
    "            x, a, y_pred  = self.forward(inputs, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(x, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[len(self.X) - 1]\n",
    "            # print progress every 500 iterations\n",
    "            if iter_num % 50 == 0:\n",
    "                print(\"\\n\\niter :%d, loss:%f\\n\" % (iter_num, smooth_loss))\n",
    "                #for i in range(generated_names):\n",
    "                    #sample_idx = self.sample()\n",
    "                    #txt = ''.join(self.data_generator.idx_to_char[idx] for idx in sample_idx)\n",
    "                #    txt = txt.title()  # capitalize first character\n",
    "                #    print ('%s' % (txt, ), end='')\n",
    "            iter_num += 1\n",
    "\n",
    "    def predict(self, start, num_sequences=1):\n",
    "        \"\"\"\n",
    "        Generate a sequence of characters using the trained self, starting from the given start sequence.\n",
    "        The generated sequence may contain a maximum of 50 characters or a newline character.\n",
    "\n",
    "        Args:\n",
    "        - start: a string containing the start sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize input vector and previous hidden state\n",
    "        x = np.zeros((self.input_size, 1))\n",
    "        x = start[0].reshape((self.input_size,1))\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        y_preds = []\n",
    "        # Generate sequence\n",
    "        for counter in range(num_sequences):\n",
    "            # Compute next hidden state and predicted character\n",
    "            a = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "            y_pred = self.softmax(np.dot(self.Wya, a) + self.by)\n",
    "            y_preds.append(y_pred)\n",
    "\n",
    "            # Update input vector, previous hidden state, and indices\n",
    "            x = np.zeros((self.input_size, 1))\n",
    "            x = start[counter].reshape((self.input_size,1))\n",
    "            a_prev = a\n",
    "\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_generator = DataGenerator('dinos.txt')\n",
    "_, x0, y0 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_1.csv\")\n",
    "_, x1, y1 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_2.csv\")\n",
    "_, x2, y2 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_3.csv\")\n",
    "_, x3, y3 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_4.csv\")\n",
    "_, x4, y4 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_5.csv\")\n",
    "_, x5, y5 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_1.csv\")\n",
    "_, x6, y6 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_2.csv\")\n",
    "_, x7, y7 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_3.csv\")\n",
    "_, x8, y8 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_4.csv\")\n",
    "_, x9, y9 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_5.csv\")\n",
    "\n",
    "x0 = x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9\n",
    "y0 = y0 + y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9\n",
    "\n",
    "sequenceLength = 10\n",
    "trainTuple, valTuple, testTuple = robot_data_treatment.createTestTrainSets(x0, y0, sequenceLength, trainRatio=0.8, valRatio=0.1, testRatio=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iter :0, loss:99.900049\n",
      "\n",
      "\n",
      "\n",
      "iter :50, loss:95.366202\n",
      "\n",
      "\n",
      "\n",
      "iter :100, loss:91.059809\n",
      "\n",
      "\n",
      "\n",
      "iter :150, loss:86.862969\n",
      "\n",
      "\n",
      "\n",
      "iter :200, loss:82.908413\n",
      "\n",
      "\n",
      "\n",
      "iter :250, loss:79.083875\n",
      "\n",
      "\n",
      "\n",
      "iter :300, loss:75.490720\n",
      "\n",
      "\n",
      "\n",
      "iter :350, loss:72.137276\n",
      "\n",
      "\n",
      "\n",
      "iter :400, loss:69.018533\n",
      "\n",
      "\n",
      "\n",
      "iter :450, loss:65.924898\n",
      "\n",
      "\n",
      "\n",
      "iter :500, loss:63.081980\n",
      "\n",
      "\n",
      "\n",
      "iter :550, loss:60.305169\n",
      "\n",
      "\n",
      "\n",
      "iter :600, loss:57.590756\n",
      "\n",
      "\n",
      "\n",
      "iter :650, loss:55.009548\n",
      "\n",
      "\n",
      "\n",
      "iter :700, loss:52.660325\n",
      "\n",
      "\n",
      "\n",
      "iter :750, loss:50.404504\n",
      "\n",
      "\n",
      "\n",
      "iter :800, loss:48.206727\n",
      "\n",
      "\n",
      "\n",
      "iter :850, loss:46.149161\n",
      "\n",
      "\n",
      "\n",
      "iter :900, loss:44.199572\n",
      "\n",
      "\n",
      "\n",
      "iter :950, loss:42.418750\n",
      "\n",
      "\n",
      "\n",
      "iter :1000, loss:40.549766\n",
      "\n",
      "\n",
      "\n",
      "iter :1050, loss:38.899486\n",
      "\n",
      "\n",
      "\n",
      "iter :1100, loss:37.362322\n",
      "\n",
      "\n",
      "\n",
      "iter :1150, loss:35.826491\n",
      "\n",
      "\n",
      "\n",
      "iter :1200, loss:34.458299\n",
      "\n",
      "\n",
      "\n",
      "iter :1250, loss:33.010498\n",
      "\n",
      "\n",
      "\n",
      "iter :1300, loss:31.749008\n",
      "\n",
      "\n",
      "\n",
      "iter :1350, loss:30.573779\n",
      "\n",
      "\n",
      "\n",
      "iter :1400, loss:29.497132\n",
      "\n",
      "\n",
      "\n",
      "iter :1450, loss:28.493527\n",
      "\n",
      "\n",
      "\n",
      "iter :1500, loss:27.400683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN_robot(trainTuple, valTuple, hidden_size=200, sequence_length=sequenceLength, learning_rate=1e-3)\n",
    "rnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3938943 ]\n",
      " [ 0.46838724]\n",
      " [-0.0167895 ]] [[-0.2727314 ]\n",
      " [ 0.36737851]\n",
      " [-0.0167895 ]]\n",
      "[[-0.3964885 ]\n",
      " [ 0.49113561]\n",
      " [-0.0167895 ]] [[-0.2719689 ]\n",
      " [ 0.42392613]\n",
      " [ 0.15241288]]\n",
      "[[-0.39572552]\n",
      " [ 0.54768275]\n",
      " [ 0.15241288]] [[-0.27091121]\n",
      " [ 0.37448657]\n",
      " [ 0.15241288]]\n",
      "[[-0.3946684 ]\n",
      " [ 0.49824376]\n",
      " [ 0.15241288]] [[-0.27278621]\n",
      " [ 0.36988657]\n",
      " [ 0.15241288]]\n",
      "[[-0.39654277]\n",
      " [ 0.49364313]\n",
      " [ 0.15241288]] [[-0.26693166]\n",
      " [ 0.37579187]\n",
      " [ 0.06995833]]\n",
      "[[-0.39068832]\n",
      " [ 0.49954853]\n",
      " [ 0.06995833]] [[-0.26511395]\n",
      " [ 0.37130229]\n",
      " [ 0.06995833]]\n",
      "[[-0.38887048]\n",
      " [ 0.49505881]\n",
      " [ 0.06995833]] [[-0.26230626]\n",
      " [ 0.33137921]\n",
      " [ 0.01682197]]\n",
      "[[-0.38606286]\n",
      " [ 0.45513581]\n",
      " [ 0.01682197]] [[-0.26256376]\n",
      " [ 0.47739588]\n",
      " [ 0.08552197]]\n",
      "[[-0.38632043]\n",
      " [ 0.60115255]\n",
      " [ 0.08552197]] [[-0.26147285]\n",
      " [ 0.4860777 ]\n",
      " [ 0.08552197]]\n"
     ]
    }
   ],
   "source": [
    "test = testTuple[14] # random input\n",
    "input = test[0]\n",
    "output = test[1]\n",
    "for i in range(len(input)-1):\n",
    "    est1, est2 = rnn.predict([input[i], input[i+1]], 2)\n",
    "    gt1 = output[i].reshape(3,1)\n",
    "    gt2 = output[i+1].reshape(3,1)\n",
    "    diff1 = gt1 - est1\n",
    "    diff2 = gt2 - est2\n",
    "    print(diff1, diff2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
