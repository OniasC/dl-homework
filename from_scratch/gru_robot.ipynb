{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import robot_data_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_generator = DataGenerator('dinos.txt')\n",
    "_, x0, y0 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_1.csv\")\n",
    "_, x1, y1 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_2.csv\")\n",
    "_, x2, y2 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_3.csv\")\n",
    "_, x3, y3 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_4.csv\")\n",
    "_, x4, y4 = robot_data_treatment.dataGet(\"../data/quadrado_opt_1_5.csv\")\n",
    "_, x5, y5 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_1.csv\")\n",
    "_, x6, y6 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_2.csv\")\n",
    "_, x7, y7 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_3.csv\")\n",
    "_, x8, y8 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_4.csv\")\n",
    "_, x9, y9 = robot_data_treatment.dataGet(\"../data/quadrado_opt_2_5.csv\")\n",
    "\n",
    "x0 = x0 + x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9\n",
    "y0 = y0 + y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9\n",
    "\n",
    "sequenceLength = 10\n",
    "trainTuple, valTuple, testTuple = robot_data_treatment.createTestTrainSets(x0, y0, sequenceLength, trainRatio=0.8, valRatio=0.1, testRatio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_robot:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (GRU).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the GR.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the GRU.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the GRU.\n",
    "    self.learning_rate : float\n",
    "        The learning rate used during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
    "        Initializes an instance of the GRU class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainSet, valSet, hidden_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the GRU class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the GRU.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the GRU.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the GRU.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.trainSet = trainSet\n",
    "        self.valSet = valSet\n",
    "\n",
    "        self.output_size = 3\n",
    "        self.input_size = 5\n",
    "\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        # model parameters\n",
    "        self.Wz = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + self.input_size))\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + self.input_size))\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + self.input_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (self.output_size, hidden_size))\n",
    "        self.by = np.zeros((self.output_size, 1))\n",
    "\n",
    "        # initialize gradients for each parameter\n",
    "        self.dWz, self.dWr, self.dWa, self.dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(\n",
    "            self.Wa), np.zeros_like(self.Wy)\n",
    "        self.dbz, self.dbr, self.dba, self.dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(\n",
    "            self.bz), np.zeros_like(self.by)\n",
    "\n",
    "        # initialize parameters for adamw optimizer\n",
    "        self.mWz = np.zeros_like(self.Wz)\n",
    "        self.vWz = np.zeros_like(self.Wz)\n",
    "        self.mWr = np.zeros_like(self.Wr)\n",
    "        self.vWr = np.zeros_like(self.Wr)\n",
    "        self.mWa = np.zeros_like(self.Wa)\n",
    "        self.vWa = np.zeros_like(self.Wa)\n",
    "        self.mWy = np.zeros_like(self.Wy)\n",
    "        self.vWy = np.zeros_like(self.Wy)\n",
    "        self.mbz = np.zeros_like(self.bz)\n",
    "        self.vbz = np.zeros_like(self.bz)\n",
    "        self.mbr = np.zeros_like(self.br)\n",
    "        self.vbr = np.zeros_like(self.br)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, c_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a simple GRU model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "\n",
    "        Returns: X (numpy array): Input sequence, shape (sequence_length, input_size) c (dictionary): Cell state for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) r (dictionary): Reset gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) z (dictionary): Update gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) cc (dictionary): Candidate cell\n",
    "        state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) a (dictionary):\n",
    "        Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) y_pred (\n",
    "        dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (\n",
    "        output_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize dictionaries for backpropagation\n",
    "        # initialize dictionaries for backpropagation\n",
    "        r, z, c, cc, a, y_pred = {}, {}, {}, {}, {}, {}\n",
    "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
    "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
    "\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(X.shape[0]):\n",
    "            # concatenate the input and hidden state\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r[t] = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z[t] = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc[t] = np.tanh(np.dot(self.Wa, np.vstack((r[t] * a[t - 1], xt))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c[t] = z[t] * cc[t] + (1 - z[t]) * c[t - 1]\n",
    "\n",
    "            # compute the hidden state\n",
    "            a[t] = c[t]\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
    "\n",
    "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
    "        return X, r, z, c, cc, a, y_pred\n",
    "\n",
    "    def backward(self, X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through time for a GRU network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "            r (dictionary): Reset gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            z (dictionary): Update gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
    "            targets (numpy array): Target outputs for each time step, shape (sequence_length, output_size)\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Initialize gradients for hidden state\n",
    "        dc_next = np.zeros_like(c_prev)\n",
    "        da_next = np.zeros_like(a_prev)\n",
    "\n",
    "        # Iterate backwards through time steps\n",
    "        for t in reversed(range(X.shape[0])):\n",
    "            # compute the gradient of the output probability vector\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # compute the gradient of the output layer weights and biases\n",
    "            self.dWy += np.dot(dy, a[t].T)\n",
    "            self.dby += dy\n",
    "\n",
    "            # compute the gradient of the hidden state\n",
    "            da = np.dot(self.Wy.T, dy) + da_next\n",
    "\n",
    "            # compute the gradient of the update gate\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a_prev, xt))\n",
    "            dz = da * (a[t] - c[t])\n",
    "            self.dWz += np.dot(dz, concat.T)\n",
    "            self.dbz += dz\n",
    "\n",
    "            # compute the gradient of the reset gate\n",
    "            dr = da * np.dot(self.Wz[:, :self.hidden_size].T, dz) * (1 - r[t]) * r[t]\n",
    "            self.dWr += np.dot(dr, concat.T)\n",
    "            self.dbr += dr\n",
    "\n",
    "            # compute the gradient of the current hidden state\n",
    "            da = np.dot(self.Wa[:, :self.hidden_size].T, dr) + np.dot(self.Wz[:, :self.hidden_size].T, dz)\n",
    "            self.dWa += np.dot(da * (1 - a[t]**2), concat.T)\n",
    "            self.dba += da * (1 - a[t]**2)\n",
    "\n",
    "            # compute the gradient of the input to the next hidden state\n",
    "            da_next = np.dot(self.Wr[:, :self.hidden_size].T, dr) \\\n",
    "                      + np.dot(self.Wz[:, :self.hidden_size].T, dz) \\\n",
    "                      + np.dot(self.Wa[:, :self.hidden_size].T, da)\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in [self.dWz, self.dWr, self.dWa, self.dWy, self.dbz, self.dbr, self.dba, self.dby]:\n",
    "            np.clip(grad, -1, 1)\n",
    "\n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the GRU's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # AdamW update for Wz\n",
    "        self.mWz = beta1 * self.mWz + (1 - beta1) * self.dWz\n",
    "        self.vWz = beta2 * self.vWz + (1 - beta2) * np.square(self.dWz)\n",
    "        m_hat = self.mWz / (1 - beta1)\n",
    "        v_hat = self.vWz / (1 - beta2)\n",
    "        self.Wz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wz)\n",
    "\n",
    "        # AdamW update for bu\n",
    "        self.mbz = beta1 * self.mbz + (1 - beta1) * self.dbz\n",
    "        self.vbz = beta2 * self.vbz + (1 - beta2) * np.square(self.dbz)\n",
    "        m_hat = self.mbz / (1 - beta1)\n",
    "        v_hat = self.vbz / (1 - beta2)\n",
    "        self.bz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bz)\n",
    "\n",
    "        # AdamW update for Wr\n",
    "        self.mWr = beta1 * self.mWr + (1 - beta1) * self.dWr\n",
    "        self.vWr = beta2 * self.vWr + (1 - beta2) * np.square(self.dWr)\n",
    "        m_hat = self.mWr / (1 - beta1)\n",
    "        v_hat = self.vWr / (1 - beta2)\n",
    "        self.Wr -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wr)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mbr = beta1 * self.mbr + (1 - beta1) * self.dbr\n",
    "        self.vbr = beta2 * self.vbr + (1 - beta2) * np.square(self.dbr)\n",
    "        m_hat = self.mbr / (1 - beta1)\n",
    "        v_hat = self.vbr / (1 - beta2)\n",
    "        self.br -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.br)\n",
    "\n",
    "        # AdamW update for Wa\n",
    "        self.mWa = beta1 * self.mWa + (1 - beta1) * self.dWa\n",
    "        self.vWa = beta2 * self.vWa + (1 - beta2) * np.square(self.dWa)\n",
    "        m_hat = self.mWa / (1 - beta1)\n",
    "        v_hat = self.vWa / (1 - beta2)\n",
    "        self.Wa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wa)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for Wy\n",
    "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
    "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
    "        m_hat = self.mWy / (1 - beta1)\n",
    "        v_hat = self.vWy / (1 - beta2)\n",
    "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "        m_hat = self.mby / (1 - beta1)\n",
    "        v_hat = self.vby / (1 - beta2)\n",
    "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
    "\n",
    "    def train(self, data_generator,iterations):\n",
    "        \"\"\"\n",
    "        Train the GRU on a dataset using backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            data_generator: An instance of DataGenerator containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        iter_num = 0\n",
    "        # stopping criterion for training\n",
    "        threshold = 50\n",
    "\n",
    "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "        while (iter_num < iterations):\n",
    "            # initialize hidden state at the beginning of each sequence\n",
    "            if data_generator.pointer == 0:\n",
    "                c_prev = np.zeros((self.hidden_size, 1))\n",
    "                a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = data_generator.next_batch()\n",
    "\n",
    "            # forward pass\n",
    "            X, r, z, c, cc, a, y_pred = self.forward(inputs, c_prev, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[self.sequence_length - 1]\n",
    "            c_prev = c[self.sequence_length - 1]\n",
    "#             if iter_num == 5900 or iter_num == 30000:\n",
    "#                         self.learning_rate *= 0.1\n",
    "            # print progress every 100 iterations\n",
    "            if iter_num % 100 == 0:\n",
    "#                 self.learning_rate *= 0.99\n",
    "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
    "                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n",
    "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "\n",
    "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "\n",
    "        Args:\n",
    "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
    "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
    "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
    "            n (int): Number of characters to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize input and seed_idx\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        # convert one-hot encoding to integer index\n",
    "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
    "\n",
    "        # set the seed letter as the input for the first time step\n",
    "        x[seed_idx] = 1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        idxes = []\n",
    "        c = np.copy(c_prev)\n",
    "        a = np.copy(a_prev)\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "            c = z * c + (1 - z) * cc\n",
    "            a = c\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # append the sampled character to the sequence\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # return the generated sequence\n",
    "        return idxes\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        \"\"\"\n",
    "        Generate a sequence of n characters using the trained GRU model, starting from the given start sequence.\n",
    "\n",
    "        Args:\n",
    "        - data_generator: an instance of DataGenerator\n",
    "        - start: a string containing the start sequence\n",
    "        - n: an integer indicating the length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "        # initialize input sequence\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        # initialize cell state and hidden state\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # generate new sequence of characters\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c = z * cc + (1 - z) * c\n",
    "\n",
    "            # compute the hidden state\n",
    "            a = c\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 24\n",
    "#read text from the \"input.txt\" file\n",
    "gru =  GRU_robot(trainTuple, valTuple, hidden_size=100, sequence_length=sequence_length,learning_rate=0.005)\n",
    "\n",
    "gru.train(iterations=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cerus thas that\\nThas bru, May pricsit reitne, band wirs pearvife he wal be lour nusto thos nhely;\\nTht biked erur,\\n\\nTh hh ppont he lalds bun dearve nuf re fay onurary sra lde my, o do dt Sererefrurgans, 'ld\\nThery.\\n\\nSCtir,\\nAy silko, mo bad our I hier nut\\nty\\nORUMINIl Ad sa'l,\\nButur, is ugssarc kouf fe fledr the wror fur sthe myo upresty,\\nHapirt shal; I pre dmel n:\\nYourperins turga, bat hi for ldu hpavi bay urver sory uplaprer fore ne yob urill kellsy uits nwor furs we waod otusr, ray urt tho he goprat lispats teld; wht hai pres rikus!\\n\\nOMaNI In uld waerus, tur bos tol my eful dos anst harad lass the ne by lass 'tr!\\n\\nThthit\\nThals moun kens by ourmpelli\\nAnis bfol the dy rary auppre nfare saky prpery.\\nd cou nd Secourkou pns thapte, d wear pat hipis flais s purave styom.\\n\\nSI for bles wod han fo brald fat enye owlld eve.\\n\\nThes fresal I whil fur haly sior ury wad athor whallve wo uels pury, wikisld naknes serend erbeny\\nA uppraknt he fur woll fe reve\\nrurnd ais bey yorur upr wit elist sally!\\n\\nThim\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = testTuple[14] # random input\n",
    "input = test[0]\n",
    "output = test[1]\n",
    "for i in range(len(input)-1):\n",
    "    est1, est2 = gru.predict([input[i], input[i+1]], 2)\n",
    "    gt1 = output[i].reshape(3,1)\n",
    "    gt2 = output[i+1].reshape(3,1)\n",
    "    diff1 = gt1 - est1\n",
    "    diff2 = gt2 - est2\n",
    "    print(diff1, diff2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
