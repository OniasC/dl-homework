{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for reading and preprocessing text data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str, sequence_length: int):\n",
    "        \"\"\"\n",
    "        Initializes a DataReader object with the path to a text file and the desired sequence length.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file.\n",
    "            sequence_length (int): The length of the sequences that will be fed to the self.\n",
    "        \"\"\"\n",
    "        with open(path) as f:\n",
    "            # Read the contents of the file\n",
    "            self.data = f.read()\n",
    "\n",
    "        # Find all unique characters in the text\n",
    "        chars = list(set(self.data))\n",
    "\n",
    "        # Create dictionaries to map characters to indices and vice versa\n",
    "        self.char_to_idx = {ch: i for (i, ch) in enumerate(chars)}\n",
    "        self.idx_to_char = {i: ch for (i, ch) in enumerate(chars)}\n",
    "\n",
    "        # Store the size of the text data and the size of the vocabulary\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "        # Initialize the pointer that will be used to generate sequences\n",
    "        self.pointer = 0\n",
    "\n",
    "        # Store the desired sequence length\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Generates a batch of input and target sequences.\n",
    "\n",
    "        Returns:\n",
    "            inputs_one_hot (np.ndarray): A numpy array with shape `(batch_size, vocab_size)` where each row is a one-hot encoded representation of a character in the input sequence.\n",
    "            targets (list): A list of integers that correspond to the indices of the characters in the target sequence, which is the same as the input sequence shifted by one position to the right.\n",
    "        \"\"\"\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.sequence_length\n",
    "\n",
    "        # Get the input sequence as a list of integers\n",
    "        inputs = [self.char_to_idx[ch] for ch in self.data[input_start:input_end]]\n",
    "\n",
    "        # One-hot encode the input sequence\n",
    "        inputs_one_hot = np.zeros((len(inputs), self.vocab_size))\n",
    "        inputs_one_hot[np.arange(len(inputs)), inputs] = 1\n",
    "\n",
    "        # Get the target sequence as a list of integers\n",
    "        targets = [self.char_to_idx[ch] for ch in self.data[input_start + 1:input_end + 1]]\n",
    "\n",
    "        # Update the pointer\n",
    "        self.pointer += self.sequence_length\n",
    "\n",
    "        # Reset the pointer if the next batch would exceed the length of the text data\n",
    "        if self.pointer + self.sequence_length + 1 >= self.data_size:\n",
    "            self.pointer = 0\n",
    "\n",
    "        return inputs_one_hot, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    \"\"\"\n",
    "    A class used to represent a Recurrent Neural Network (GRU).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    hidden_size : int\n",
    "        The number of hidden units in the GR.\n",
    "    vocab_size : int\n",
    "        The size of the vocabulary used by the GRU.\n",
    "    sequence_length : int\n",
    "        The length of the input sequences fed to the GRU.\n",
    "    self.learning_rate : float\n",
    "        The learning rate used during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __init__(hidden_size, vocab_size, sequence_length, self.learning_rate)\n",
    "        Initializes an instance of the GRU class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, vocab_size, sequence_length, learning_rate):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the GRU class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        hidden_size : int\n",
    "            The number of hidden units in the GRU.\n",
    "        vocab_size : int\n",
    "            The size of the vocabulary used by the GRU.\n",
    "        sequence_length : int\n",
    "            The length of the input sequences fed to the GRU.\n",
    "        learning_rate : float\n",
    "            The learning rate used during training.\n",
    "        \"\"\"\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # model parameters\n",
    "        self.Wz = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.bz = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wr = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.br = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wa = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (hidden_size, hidden_size + vocab_size))\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wy = np.random.uniform(-np.sqrt(1. / hidden_size), np.sqrt(1. / hidden_size),\n",
    "                                    (vocab_size, hidden_size))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "\n",
    "        # initialize gradients for each parameter\n",
    "        self.dWz, self.dWr, self.dWa, self.dWy = np.zeros_like(self.Wz), np.zeros_like(self.Wr), np.zeros_like(\n",
    "            self.Wa), np.zeros_like(self.Wy)\n",
    "        self.dbz, self.dbr, self.dba, self.dby = np.zeros_like(self.bz), np.zeros_like(self.br), np.zeros_like(\n",
    "            self.bz), np.zeros_like(self.by)\n",
    "\n",
    "        # initialize parameters for adamw optimizer\n",
    "        self.mWz = np.zeros_like(self.Wz)\n",
    "        self.vWz = np.zeros_like(self.Wz)\n",
    "        self.mWr = np.zeros_like(self.Wr)\n",
    "        self.vWr = np.zeros_like(self.Wr)\n",
    "        self.mWa = np.zeros_like(self.Wa)\n",
    "        self.vWa = np.zeros_like(self.Wa)\n",
    "        self.mWy = np.zeros_like(self.Wy)\n",
    "        self.vWy = np.zeros_like(self.Wy)\n",
    "        self.mbz = np.zeros_like(self.bz)\n",
    "        self.vbz = np.zeros_like(self.bz)\n",
    "        self.mbr = np.zeros_like(self.br)\n",
    "        self.vbr = np.zeros_like(self.br)\n",
    "        self.mba = np.zeros_like(self.ba)\n",
    "        self.vba = np.zeros_like(self.ba)\n",
    "        self.mby = np.zeros_like(self.by)\n",
    "        self.vby = np.zeros_like(self.by)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Computes the sigmoid activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the sigmoid activation values.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax activation function for a given input array.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Array of the same shape as `x`, containing the softmax activation values.\n",
    "        \"\"\"\n",
    "        # shift the input to prevent overflow when computing the exponentials\n",
    "        x = x - np.max(x)\n",
    "        # compute the exponentials of the shifted input\n",
    "        p = np.exp(x)\n",
    "        # normalize the exponentials by dividing by their sum\n",
    "        return p / np.sum(p)\n",
    "\n",
    "    def forward(self, X, c_prev, a_prev):\n",
    "        \"\"\"\n",
    "        Performs forward propagation for a simple GRU model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            c_prev (numpy array): Previous cell state, shape (hidden_size, 1)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "\n",
    "        Returns: X (numpy array): Input sequence, shape (sequence_length, input_size) c (dictionary): Cell state for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) r (dictionary): Reset gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) z (dictionary): Update gate for\n",
    "        each time step, keys = time step, values = numpy array shape (hidden_size, 1) cc (dictionary): Candidate cell\n",
    "        state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) a (dictionary):\n",
    "        Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1) y_pred (\n",
    "        dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (\n",
    "        output_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize dictionaries for backpropagation\n",
    "        # initialize dictionaries for backpropagation\n",
    "        r, z, c, cc, a, y_pred = {}, {}, {}, {}, {}, {}\n",
    "        c[-1] = np.copy(c_prev)  # store the initial cell state in the dictionary\n",
    "        a[-1] = np.copy(a_prev)  # store the initial hidden state in the dictionary\n",
    "\n",
    "        # iterate over each time step in the input sequence\n",
    "        for t in range(X.shape[0]):\n",
    "            # concatenate the input and hidden state\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a[t - 1], xt))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r[t] = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z[t] = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc[t] = np.tanh(np.dot(self.Wa, np.vstack((r[t] * a[t - 1], xt))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c[t] = z[t] * cc[t] + (1 - z[t]) * c[t - 1]\n",
    "\n",
    "            # compute the hidden state\n",
    "            a[t] = c[t]\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred[t] = self.softmax(np.dot(self.Wy, a[t]) + self.by)\n",
    "\n",
    "        # return the output probability vectors, cell state, hidden state and gate vectors\n",
    "        return X, r, z, c, cc, a, y_pred\n",
    "\n",
    "    def backward(self, X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets):\n",
    "        \"\"\"\n",
    "        Performs backward propagation through time for a GRU network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy array): Input sequence, shape (sequence_length, input_size)\n",
    "            a_prev (numpy array): Previous hidden state, shape (hidden_size, 1)\n",
    "            r (dictionary): Reset gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            z (dictionary): Update gate for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            c (dictionary): Cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            cc (dictionary): Candidate cell state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            a (dictionary): Hidden state for each time step, keys = time step, values = numpy array shape (hidden_size, 1)\n",
    "            y_pred (dictionary): Output probability vector for each time step, keys = time step, values = numpy array shape (output_size, 1)\n",
    "            targets (numpy array): Target outputs for each time step, shape (sequence_length, output_size)\n",
    "\n",
    "        Returns:\n",
    "            None       \n",
    "        \"\"\"\n",
    "        # Initialize gradients for hidden state\n",
    "        dc_next = np.zeros_like(c_prev)\n",
    "        da_next = np.zeros_like(a_prev)\n",
    "\n",
    "        # Iterate backwards through time steps\n",
    "        for t in reversed(range(X.shape[0])):\n",
    "            # compute the gradient of the output probability vector\n",
    "            dy = np.copy(y_pred[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # compute the gradient of the output layer weights and biases\n",
    "            self.dWy += np.dot(dy, a[t].T)\n",
    "            self.dby += dy\n",
    "\n",
    "            # compute the gradient of the hidden state\n",
    "            da = np.dot(self.Wy.T, dy) + da_next\n",
    "\n",
    "            # compute the gradient of the update gate\n",
    "            xt = X[t, :].reshape(-1, 1)\n",
    "            concat = np.vstack((a_prev, xt))\n",
    "            dz = da * (a[t] - c[t])\n",
    "            self.dWz += np.dot(dz, concat.T)\n",
    "            self.dbz += dz\n",
    "\n",
    "            # compute the gradient of the reset gate\n",
    "            dr = da * np.dot(self.Wz[:, :self.hidden_size].T, dz) * (1 - r[t]) * r[t]\n",
    "            self.dWr += np.dot(dr, concat.T)\n",
    "            self.dbr += dr\n",
    "\n",
    "            # compute the gradient of the current hidden state\n",
    "            da = np.dot(self.Wa[:, :self.hidden_size].T, dr) + np.dot(self.Wz[:, :self.hidden_size].T, dz)\n",
    "            self.dWa += np.dot(da * (1 - a[t]**2), concat.T)\n",
    "            self.dba += da * (1 - a[t]**2)\n",
    "\n",
    "            # compute the gradient of the input to the next hidden state\n",
    "            da_next = np.dot(self.Wr[:, :self.hidden_size].T, dr) \\\n",
    "                      + np.dot(self.Wz[:, :self.hidden_size].T, dz) \\\n",
    "                      + np.dot(self.Wa[:, :self.hidden_size].T, da)\n",
    "        # clip gradients to avoid exploding gradients\n",
    "        for grad in [self.dWz, self.dWr, self.dWa, self.dWy, self.dbz, self.dbr, self.dba, self.dby]:\n",
    "            np.clip(grad, -1, 1)\n",
    "\n",
    "    def loss(self, y_preds, targets):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss for a given sequence of predicted probabilities and true targets.\n",
    "\n",
    "        Parameters:\n",
    "            y_preds (ndarray): Array of shape (sequence_length, vocab_size) containing the predicted probabilities for each time step.\n",
    "            targets (ndarray): Array of shape (sequence_length, 1) containing the true targets for each time step.\n",
    "\n",
    "        Returns:\n",
    "            float: Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        # calculate cross-entropy loss\n",
    "        return sum(-np.log(y_preds[t][targets[t], 0]) for t in range(self.sequence_length))\n",
    "\n",
    "    def adamw(self, beta1=0.9, beta2=0.999, epsilon=1e-8, L2_reg=1e-4):\n",
    "        \"\"\"\n",
    "        Updates the GRU's parameters using the AdamW optimization algorithm.\n",
    "        \"\"\"\n",
    "        \n",
    "        # AdamW update for Wz\n",
    "        self.mWz = beta1 * self.mWz + (1 - beta1) * self.dWz\n",
    "        self.vWz = beta2 * self.vWz + (1 - beta2) * np.square(self.dWz)\n",
    "        m_hat = self.mWz / (1 - beta1)\n",
    "        v_hat = self.vWz / (1 - beta2)\n",
    "        self.Wz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wz)\n",
    "\n",
    "        # AdamW update for bu\n",
    "        self.mbz = beta1 * self.mbz + (1 - beta1) * self.dbz\n",
    "        self.vbz = beta2 * self.vbz + (1 - beta2) * np.square(self.dbz)\n",
    "        m_hat = self.mbz / (1 - beta1)\n",
    "        v_hat = self.vbz / (1 - beta2)\n",
    "        self.bz -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.bz)\n",
    "\n",
    "        # AdamW update for Wr\n",
    "        self.mWr = beta1 * self.mWr + (1 - beta1) * self.dWr\n",
    "        self.vWr = beta2 * self.vWr + (1 - beta2) * np.square(self.dWr)\n",
    "        m_hat = self.mWr / (1 - beta1)\n",
    "        v_hat = self.vWr / (1 - beta2)\n",
    "        self.Wr -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wr)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mbr = beta1 * self.mbr + (1 - beta1) * self.dbr\n",
    "        self.vbr = beta2 * self.vbr + (1 - beta2) * np.square(self.dbr)\n",
    "        m_hat = self.mbr / (1 - beta1)\n",
    "        v_hat = self.vbr / (1 - beta2)\n",
    "        self.br -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.br)\n",
    "\n",
    "        # AdamW update for Wa\n",
    "        self.mWa = beta1 * self.mWa + (1 - beta1) * self.dWa\n",
    "        self.vWa = beta2 * self.vWa + (1 - beta2) * np.square(self.dWa)\n",
    "        m_hat = self.mWa / (1 - beta1)\n",
    "        v_hat = self.vWa / (1 - beta2)\n",
    "        self.Wa -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wa)\n",
    "\n",
    "        # AdamW update for br\n",
    "        self.mba = beta1 * self.mba + (1 - beta1) * self.dba\n",
    "        self.vba = beta2 * self.vba + (1 - beta2) * np.square(self.dba)\n",
    "        m_hat = self.mba / (1 - beta1)\n",
    "        v_hat = self.vba / (1 - beta2)\n",
    "        self.ba -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.ba)\n",
    "\n",
    "        # AdamW update for Wy\n",
    "        self.mWy = beta1 * self.mWy + (1 - beta1) * self.dWy\n",
    "        self.vWy = beta2 * self.vWy + (1 - beta2) * np.square(self.dWy)\n",
    "        m_hat = self.mWy / (1 - beta1)\n",
    "        v_hat = self.vWy / (1 - beta2)\n",
    "        self.Wy -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.Wy)\n",
    "\n",
    "        # AdamW update for by\n",
    "        self.mby = beta1 * self.mby + (1 - beta1) * self.dby\n",
    "        self.vby = beta2 * self.vby + (1 - beta2) * np.square(self.dby)\n",
    "        m_hat = self.mby / (1 - beta1)\n",
    "        v_hat = self.vby / (1 - beta2)\n",
    "        self.by -= self.learning_rate * (m_hat / (np.sqrt(v_hat) + epsilon) + L2_reg * self.by)\n",
    "\n",
    "    def train(self, data_generator,iterations):\n",
    "        \"\"\"\n",
    "        Train the GRU on a dataset using backpropagation through time.\n",
    "\n",
    "        Args:\n",
    "            data_generator: An instance of DataGenerator containing the training data.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        iter_num = 0\n",
    "        # stopping criterion for training\n",
    "        threshold = 50\n",
    "    \n",
    "        smooth_loss = -np.log(1.0 / data_generator.vocab_size) * self.sequence_length  # initialize loss\n",
    "        while (iter_num < iterations):\n",
    "            # initialize hidden state at the beginning of each sequence\n",
    "            if data_generator.pointer == 0:\n",
    "                c_prev = np.zeros((self.hidden_size, 1))\n",
    "                a_prev = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "            # get a batch of inputs and targets\n",
    "            inputs, targets = data_generator.next_batch()\n",
    "\n",
    "            # forward pass\n",
    "            X, r, z, c, cc, a, y_pred = self.forward(inputs, c_prev, a_prev)\n",
    "\n",
    "            # backward pass\n",
    "            self.backward(X, a_prev, c_prev, r, z, c, cc, a, y_pred, targets)\n",
    "\n",
    "            # calculate and update loss\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            self.adamw()\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            # update previous hidden state for the next batch\n",
    "            a_prev = a[self.sequence_length - 1]\n",
    "            c_prev = c[self.sequence_length - 1]\n",
    "#             if iter_num == 5900 or iter_num == 30000:\n",
    "#                         self.learning_rate *= 0.1\n",
    "            # print progress every 100 iterations\n",
    "            if iter_num % 100 == 0:\n",
    "#                 self.learning_rate *= 0.99\n",
    "                sample_idx = self.sample(c_prev, a_prev, inputs[0, :], 200)\n",
    "                print(''.join(data_generator.idx_to_char[idx] for idx in sample_idx))\n",
    "                print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "\n",
    "    def sample(self, c_prev, a_prev, seed_idx, n):\n",
    "        \"\"\"\n",
    "        Sample a sequence of integers from the model.\n",
    "\n",
    "        Args:\n",
    "            c_prev (numpy.ndarray): Previous cell state, a numpy array of shape (hidden_size, 1).\n",
    "            a_prev (numpy.ndarray): Previous hidden state, a numpy array of shape (hidden_size, 1).\n",
    "            seed_idx (numpy.ndarray): Seed letter from the first time step, a numpy array of shape (vocab_size, 1).\n",
    "            n (int): Number of characters to generate.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integers representing the generated sequence.\n",
    "\n",
    "        \"\"\"\n",
    "        # initialize input and seed_idx\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        # convert one-hot encoding to integer index\n",
    "        seed_idx = np.argmax(seed_idx, axis=-1)\n",
    "\n",
    "        # set the seed letter as the input for the first time step\n",
    "        x[seed_idx] = 1\n",
    "\n",
    "        # generate sequence of characters\n",
    "        idxes = []\n",
    "        c = np.copy(c_prev)\n",
    "        a = np.copy(a_prev)\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "            c = z * c + (1 - z) * cc\n",
    "            a = c\n",
    "            # compute the output probabilities\n",
    "            y = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y.ravel())\n",
    "\n",
    "            # set the input for the next time step\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "            # append the sampled character to the sequence\n",
    "            idxes.append(idx)\n",
    "\n",
    "        # return the generated sequence\n",
    "        return idxes\n",
    "\n",
    "    def predict(self, data_generator, start, n):\n",
    "        \"\"\"\n",
    "        Generate a sequence of n characters using the trained GRU model, starting from the given start sequence.\n",
    "\n",
    "        Args:\n",
    "        - data_generator: an instance of DataGenerator\n",
    "        - start: a string containing the start sequence\n",
    "        - n: an integer indicating the length of the generated sequence\n",
    "\n",
    "        Returns:\n",
    "        - txt: a string containing the generated sequence\n",
    "        \"\"\"\n",
    "        # initialize input sequence\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        idxes = []\n",
    "        for i in range(len(chars)):\n",
    "            idx = data_generator.char_to_idx[chars[i]]\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        # initialize cell state and hidden state\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        # generate new sequence of characters\n",
    "        for t in range(n):\n",
    "            # compute the hidden state and cell state\n",
    "            concat = np.vstack((a, x))\n",
    "\n",
    "            # compute the reset gate\n",
    "            r = self.sigmoid(np.dot(self.Wr, concat) + self.br)\n",
    "\n",
    "            # compute the update gate\n",
    "            z = self.sigmoid(np.dot(self.Wz, concat) + self.bz)\n",
    "\n",
    "            # compute the candidate cell state\n",
    "            cc = np.tanh(np.dot(self.Wa, np.vstack((r * a, x))) + self.ba)\n",
    "\n",
    "            # compute the cell state\n",
    "            c = z * cc + (1 - z) * c\n",
    "\n",
    "            # compute the hidden state\n",
    "            a = c\n",
    "\n",
    "            # compute the output probability vector\n",
    "            y_pred = self.softmax(np.dot(self.Wy, a) + self.by)\n",
    "            # sample the next character from the output probabilities\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "            idxes.append(idx)\n",
    "        txt = ''.join(data_generator.idx_to_char[i] for i in idxes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H,U..zjLWI3j-UR'nlGfkjB&hhPJPTp&U'qRLbeV?$fIs&PZ:uHRQQNWALLJDa;b&RmNX.lq!;;KxuBgXoQGZcFNVa.nnaFeXIHWUmMxq'&H\n",
      "B!YPfhHC:s?rn;:aZMQya\n",
      "zplJj,3SvgTn;LnjXDqVaTdXy\n",
      "JIwKJcdLbQxhXHkW&WPh'xGDlnSn?vJ:RHCvOYZqjaO\n",
      "\n",
      "\n",
      "iter :0, loss:100.185286\n",
      "g aes ttyntienaityutiledHhre ro  titsuteeol\n",
      "e cwH\n",
      "koitiihtnhgBt aim,k;rtyc\n",
      "VrWct\n",
      "o we t\n",
      "o neo heynss rar aehltiene nodrsa uoen t\n",
      "it?\n",
      "i.nrhs foueakkjoce\n",
      " e aaike\n",
      "yib  ei r htth\n",
      "Ihi ois inoontaenetdhtic\n",
      "\n",
      "\n",
      "iter :100, loss:98.257050\n",
      " us,\n",
      "e atthgce a ar tothYYfie h ay wleei thsayeeerofdop ous ns tatndtoui fhntie te a ora i col ne eore c eore  \n",
      " we r o ninit oenng ay tollyvu egtl.s\n",
      "ieer otue wOsaoudan\n",
      "ee waoum aaaemdis nt muss fyou\n",
      "\n",
      "\n",
      "iter :200, loss:95.729453\n",
      "hre tg hat CugY$gd aene  wsye  far eeoeris tthor,l s,thi ep:\n",
      "\n",
      "t hey te,hel b wthe dir c,aner,te iti e mrhe, b mire  b,sSl hest ttheranddw pbyirourm wbyousllll.ius,henanly comree th benddi o bsMl e ae \n",
      "\n",
      "\n",
      "iter :300, loss:93.149231\n",
      "indae bto mdingt halellnell yo wogihennde ali ihdotbo\n",
      "ey\n",
      "lyl ofouhe  os\n",
      " ouo ufori ns.rel dllls, anris,t heel ellyain thli alllllyo urs,s ble hann gtthe or oudgizthe athei ors, s bormr.\n",
      "\n",
      "W\n",
      "Wsin wodfdo\n",
      "\n",
      "\n",
      "iter :400, loss:90.819237\n",
      ":\n",
      "he what anns. thakinn:\n",
      " wthel oud\n",
      "h borseccou omer eak'hrer po angsithathi,t he oneo' hthe thi tht\n",
      "\n",
      "Wearfingrwan?intonours thev\n",
      "wthhe wpall:Oe.\n",
      "T\n",
      "\n",
      "TTf's o w' theainastors atnge:\n",
      "TWendan yousr,coousr\n",
      "\n",
      "\n",
      "iter :500, loss:88.885315\n",
      "s.\n",
      "F\n",
      "ATStidis youwlas.\n",
      "Th fwietr ing hato thsato\n",
      "MhouEng\n",
      "AMNofour gake n\n",
      "'ENIe isme\n",
      "\n",
      "FT CmaraNng:\n",
      "TIhe alyou uray,Mr wituhnto hati,\n",
      "TS:\n",
      "T\n",
      "AiizNreecri mars rourae nngooratulans:.\n",
      "\n",
      "S\n",
      "ATgoour'san isse.\n",
      "n\n",
      "\n",
      "\n",
      "iter :600, loss:86.831615\n",
      "oTm:\n",
      "h '\n",
      "Mhi wtirsr th ise won;  ws mspead yoem? nes t temr meyerserem?es?\n",
      "S\n",
      "MT meree.\n",
      "SIy' IIUSn: ymakIUSeneco modweee ry pouore mou\n",
      "MENIt hesan isnn: nd he ve ingee sarer oikr ipaed ant higtge'.\n",
      "IUS\n",
      "\n",
      "\n",
      "iter :700, loss:85.421744\n",
      "ke an bse em\n",
      "NIUS:\n",
      "Tml do hedi foreds;e hale s CaIUS:\n",
      "S:\n",
      "TSI:\n",
      "MENIUS:\n",
      "MNIUS:\n",
      "S:\n",
      "TS:\n",
      "T\n",
      "MT and\n",
      "Cdte mbadsel poud hd his here mandIU han thel; ha t anddCi ns; Is! bur ay use poo fheis medh asu.\n",
      "Tw frmyu \n",
      "\n",
      "\n",
      "iter :800, loss:84.445589\n",
      " ad heret\n",
      " Cn:\n",
      "IIne your ffor mat Iras t haeld he welldte IIt ghaldo of hia m:\n",
      "\n",
      "So fon: h,In Il youes al l adse sinot no hion micuke arat ha adyo ome\n",
      "NIe at foue han fy'uk,N IIUS:\n",
      "OT\n",
      "SENNIIUS:\n",
      " pell t\n",
      "\n",
      "\n",
      "iter :900, loss:82.634631\n",
      "\n",
      "IS.\n",
      "O\n",
      "Aul f the athedseo mem bell yo'd IIUS:\n",
      "IIIUS:\n",
      "\n",
      "ONeyousl.\n",
      "IS:\n",
      "\n",
      "Th ot rhe asy uop thes hiusp;beser,d\n",
      "AR Ian;d,\n",
      "Oof dll higSNIel svearn gairet nowh and,\n",
      "IUS:\n",
      "ARsi;llis thesenodr ms:\n",
      "\n",
      "OLMENII th be\n",
      "\n",
      "\n",
      "iter :1000, loss:80.871237\n",
      "ou rethros,\n",
      "MHIr,,\n",
      "Y\n",
      "OIont hme,\n",
      "TIASn:\n",
      "\n",
      "FCis\n",
      "t\n",
      "hiutuIrf tesowlvees;ef saorciust.\n",
      "\n",
      "ARIUS:\n",
      "O\n",
      "ARt anolel dI,\n",
      "Femand reers is usoft ters toub ht here,isen:\n",
      "\n",
      "AI\n",
      "MARRIUS:\n",
      "WhIt Illad, bl ote gfor ut, thert,y\n",
      "\n",
      "\n",
      "iter :1100, loss:79.203025\n",
      "ngol Icuratn the secnut.\n",
      "\n",
      "Haeis tthimuh nes iut,\n",
      "ACitur.\n",
      "\n",
      "Heeris.\n",
      "\n",
      "HARF\n",
      "Fs!\n",
      "\n",
      "O\n",
      "ONIIUS::\n",
      "\n",
      "HIUS:\n",
      "HSe ngo.\n",
      "hillowarencist, u\n",
      "ht tall ivet, hinr, wdaris firust,\n",
      "FI:\n",
      "S hat thitu shisct hoirvrlaveen\n",
      "\n",
      "ARr in\n",
      "\n",
      "\n",
      "iter :1200, loss:77.659206\n",
      " fhar myan, heal ivebent ik eserm, uthe: le!a my.e ave\n",
      " othenn to haen yaveier atr wooureth arcekr adisse wfindi\n",
      " fen they uso, thhaveecn daveer! ast wer fi veae ravereted.\n",
      "\n",
      "BO IR alde he amese paver,\n",
      "\n",
      "\n",
      "iter :1300, loss:76.054075\n",
      "SS:\n",
      "H onrety to Cth mamye par eenes ps gave wanchy oen sep----\n",
      "ARCIIAn: onond d you, the marades dise arsrdit hedf prfa!s a le fard ather fomn fo bees chaveer thg earse chave dt, yuas palden owu\n",
      "Wht o\n",
      "\n",
      "\n",
      "iter :1400, loss:74.866342\n",
      "h you.\n",
      "oWeeld' e ra tilsesp\n",
      "WARI mapeper hey  porike my od oru proe od bon ld omean thaen you o\n",
      "RC mse! matk h amin ghand iseer geh owebe bey t,hy om lde arme onoth amse,\n",
      "ARCminddat y om INIUS: th es \n",
      "\n",
      "\n",
      "iter :1500, loss:73.742688\n",
      "ame thaincout yef sof orcon an' t nod mend tofohe myoutk may owe omfonef it sowARm Iy couMole, s poalld of iten cove nodo moe mtyo,\n",
      "W'rinde ive sisod\n",
      "Ln than sw it ntey oper aye- phaer cowilma.\n",
      "He thy\n",
      "\n",
      "\n",
      "iter :1600, loss:72.837551\n",
      "b.\n",
      "Whma ben thy wort thod onour but beilld bot.\n",
      "Whout gaswifen ber on' s wlilke thciga\n",
      "nour t yous,\n",
      "MENIUS:\n",
      "\n",
      "MRCIUS:\n",
      "L\n",
      "MENENIARUS:\n",
      "ARRCorp tiol angicencousithe tigonAgo;\n",
      "\n",
      "VLha curt's whany I m ntoure.\n",
      "\n",
      "\n",
      "iter :1700, loss:72.058625\n",
      "a serit th ast hoorye lofef onewge as\n",
      "\n",
      "MARCIUS: lt obher\n",
      "Vindi, ses sas cno kelst ty.\n",
      "M bbehes wisoru sthes atlililliasp\n",
      "Th,er it stot'r: hit.\n",
      "\n",
      "VhVont palls ibas dings tisthehes mofl, y orame!\n",
      "Latyo b\n",
      "\n",
      "\n",
      "iter :1800, loss:71.261456\n",
      " ha cilliss, fReb coreamy Than.\n",
      "V\n",
      "MARAtapeinme, werses othesr\n",
      "ha chamy mendke?\n",
      "BRUS:\n",
      "VOLMARCIIA:\n",
      "L\n",
      "LMacinther gthower theangrke.\n",
      "Tonefithe sofa,l llil:\n",
      "VOLMashIAuck etharlisent, halefrdm dea ingingaor\n",
      "\n",
      "\n",
      "iter :1900, loss:70.627876\n",
      "nt thAcanngtit-\n",
      "TMat't's\n",
      "\n",
      "VLIr:\n",
      "Thurtye hers:\n",
      "\n",
      "VOLVkMeend, hem ondepre aty oerse fous,\n",
      "Tncoke ane, weor bera ll! Men canorubelllellke: he th all\n",
      "VaLlepan courcees noghee she\n",
      "Thouto wae sthe?\n",
      "Thin, chR\n",
      "\n",
      "\n",
      "iter :2000, loss:69.907251\n",
      "sew topre fored at hours al thee, thielllbe-\n",
      "Vnotgorfem mefith f IARCIUS:\n",
      "Bof om\n",
      "Thelbe aceranthes' her ater sh lefa veles, at's es hengour thetheive, cirminges he fie thse thiengre tto bancok oweathe\n",
      "\n",
      "\n",
      "iter :2100, loss:69.214124\n",
      "e the e tiforon Ir:\n",
      "\n",
      "Thot hyo his epeve mare th ank tan tho pTh hime sonk eses th mpost oundhas fefas willallit;at?\n",
      "\n",
      "COLILIA:\n",
      "COe th th meint bely ppor at's, Ynod fir:\n",
      "'s, The veollled fe itteom seave\n",
      "\n",
      "\n",
      "iter :2200, loss:68.726589\n",
      "out- stotouAndd ih mo billlit hatt acke myo! m hey pour' Ts\n",
      "of pot;ayo pre etho ly os, Atitor.\n",
      "BUS:\n",
      "O MALIAn th po olfar chou be leind be oove tatreh om tor\n",
      "T I'lgithe ldiou t Ino nolov Macke epaolave\n",
      "\n",
      "\n",
      "iter :2300, loss:67.916651\n",
      "ne:\n",
      "\n",
      "YO, ams bu ngeved ypour toer dss beand ato he ls:\n",
      "\n",
      "CO, Mandas feg than wa sh melatk, RCIU:\n",
      "\n",
      "FO I s mba y bed hee pancos ll- k Ie: hat selit asti mal ist hadusim, cham, I mak inos trar ials td ifo\n",
      "\n",
      "\n",
      "iter :2400, loss:67.493259\n",
      "cak s but igripe ator min thut;de ve oros be totaiss menond at omyo mire men tilals wit t'ldist bed Iandu tigarv ber indivein'lp r is'd in eads ome pome the git hils t, et insende, bullas: tor wtie.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iter :2500, loss:66.841332\n",
      "t hme in.\n",
      "\n",
      "COO Manct.\n",
      "\n",
      "YNUS: hen.\n",
      "\n",
      "BUSe:\n",
      "And win. I vere nde.\n",
      "\n",
      "COOINUS:\n",
      "CO, h im burt igdorike.\n",
      "I Ay ucatr\n",
      "Hatyo enowd ho t hpend wiru: you wh iru st heas avend eimer\n",
      "O, RTUS: witiz nend war\n",
      "anthiemas\n",
      "\n",
      "\n",
      "iter :2600, loss:66.310070\n",
      "nto n\n",
      "yowoburs oeno wher bed sak, wire icoutin, wisuts: n dhithe sed, ed non whiully, tay, whist hy icon dube.\n",
      "\n",
      "CO, and ses ifvet.\n",
      "CIO I heives, ot wish wo hen teman\n",
      "obutizng.\n",
      "ENENENINUS:\n",
      "Here acsanci\n",
      "\n",
      "\n",
      "iter :2700, loss:65.808394\n",
      "ersecens, th sicensust, wicesautr, youg hiveste,\n",
      "Th onore aveuldy, hinle artsouc, I condece:n you yougore yonoros firy, h's ngiry has.\n",
      "Held.\n",
      "eownehut. he cavenot irenobul, yoo wh heury, yo wha meven.\n",
      "\n",
      "\n",
      "\n",
      "iter :2800, loss:65.388224\n",
      "cenoucetoully?\n",
      "natond, ans, as tyu, y ullould aingthe owhy cos hirs firenorem; iceincesit,\n",
      "Thas hiteore:\n",
      "\n",
      "Wha farle dgnoren ak yo ofro dangearvedee\n",
      "nons whast haly he othe.\n",
      "Thes to, chant hyou cosol f\n",
      "\n",
      "\n",
      "iter :2900, loss:64.643124\n",
      "stt?\n",
      "A\n",
      "Ftsiner thane\n",
      "not bet his co,led beuthe blyod oudenropllyo,\n",
      "Th atser:\n",
      "fat, ayould ferand goicem hatiz gllyo havereh ed\n",
      "Har ansteroly, avizenof arious.\n",
      "INUSINUS:\n",
      "SININUS: fof pot haned owl heem,\n",
      "\n",
      "\n",
      "iter :3000, loss:64.330305\n",
      "s\n",
      "erout focemy?\n",
      "Tyo lecono ds hentes,\n",
      "\n",
      "CORIUS:\n",
      "Whitheth yoully pr fov haun dorwe thauts asift hicanute fomb of yonsur.\n",
      "Se\n",
      "RIDILAnsiulat ham mneple?\n",
      "Anot\n",
      "Se?\n",
      "\n",
      "Wovepitecont.\n",
      "Se\n",
      "Turo methure chas plat an\n",
      "\n",
      "\n",
      "iter :3100, loss:64.021967\n",
      "fre the ot oweor pellcet?\n",
      "\n",
      "Tho for buthom pallive hay'd 's dd won, siche mingoth thel the pat duadd y ourthe pa's doed bererevite fer opesr d ak dsu che bet foren tre pared, Whelete\n",
      "Them bensi the nat\n",
      "\n",
      "\n",
      "iter :3200, loss:63.751570\n",
      "elin gtha tir ber'ter?\n",
      "\n",
      "Ant tha pred tak esibu biuns,\n",
      "T' thit\n",
      "bhualiz no\n",
      "es ples peorpee for yo may, t dor'\n",
      "Thit otily, ous t,\n",
      "Wee f yo, be and,\n",
      "Wane tot arpeeplsene par'tr mans pe\n",
      "ppatled wer hese ma\n",
      "\n",
      "\n",
      "iter :3300, loss:63.432415\n",
      "w, war tthe d nte notiz theeple'?\n",
      "\n",
      "PTisos wiche n's t Cirest, ons opres, terominewo\n",
      "sa wire chenbe 'd bobe thane ser teinbut on'd t ithin sot ople tpir\n",
      "Seemak toveperm,\n",
      "Thilnwens, we ope om sed nonow,\n",
      "\n",
      "\n",
      "iter :3400, loss:63.096658\n",
      "em mem.\n",
      "\n",
      "Sbed nte insagin sour'd d\n",
      "Crined oweirond cethe sat tod int ren whe ower the mo r't! t\n",
      "Se\n",
      "fermuld\n",
      "Pitz\n",
      "Seses splee ppot hes popri's bu inme',\n",
      "S\n",
      "BRTUS: Hird ghe meifdr!\n",
      "\n",
      "Crives tere' dsis oped\n",
      "\n",
      "\n",
      "iter :3500, loss:62.994210\n",
      "om cit aku god that is themant, th memis re orus eak' dit ode atre- wot hero mor gimat tee po mou!\n",
      "\n",
      "wo the mnud inses sa ullit hard!in tit anuriss se avids, CI' d the o' dt,\n",
      "Whead\n",
      "\n",
      "Sias, d younn!\n",
      "\n",
      "DID\n",
      "\n",
      "\n",
      "iter :3600, loss:62.692420\n",
      "se keneart anakd or anuss na ven\n",
      "\n",
      "Seinoty,- ucout csd\n",
      "ow, werr dunss adk, ot\n",
      "The?\n",
      "\n",
      "DIDIOLALANour!\n",
      "WCI nonised\n",
      "seancino,\n",
      "An\n",
      "Smak teno'se bald\n",
      "Th, at them os we moper de!\n",
      "\n",
      "Whald,\n",
      "COMICIOLAt Cpat tiuses \n",
      "\n",
      "\n",
      "iter :3700, loss:62.728640\n",
      "Lt\n",
      "ENot the of algs hato ghat the,\n",
      "\n",
      "CIOLANUSIIOLAn InwUS:\n",
      "IINIIO ILAt has wad\n",
      "CRIOIOL Mant-\n",
      "C\n",
      "IRIOLAtu\n",
      "Ans the balg the cout s agve q ofoun the the nar.\n",
      "\n",
      "CRIOILANIUS:\n",
      "\n",
      "CIIOLA dCgamonad ghe otu th mold\n",
      "\n",
      "\n",
      "iter :3800, loss:62.600820\n",
      "cheneximean'd she mat yous wotheace allk the.\n",
      "MENIUS:\n",
      "ACIOLMINIUS\n",
      "IOLANoUS:\n",
      "II CIOLANUS:\n",
      "IOLANUUS:\n",
      "ILAn onert cerecon.\n",
      "\n",
      "CIOLANUS:\n",
      "\n",
      "bRIUS:\n",
      "ICOLAUS:\n",
      "MENIUS:\n",
      "CIOOLANIUS:\n",
      "IORICI 'd ome.\n",
      "\n",
      "DIOLAt:\n",
      "o hpeocin\n",
      "\n",
      "\n",
      "iter :3900, loss:62.212311\n",
      "\n",
      "\n",
      "ACOLAn gino thou ht ercenous.\n",
      "\n",
      "Ctine.\n",
      "I thehe cerileh thegacinan gtha malke.\n",
      "\n",
      "ENEINUS:\n",
      "'lee thes qerof cermibut naug that ghe heen ter the she cthe veerd:\n",
      "Have the cocines.\n",
      "\n",
      "MENIUS:\n",
      "\n",
      "INIOLANUS:\n",
      "IOLA\n",
      "\n",
      "\n",
      "iter :4000, loss:62.050784\n",
      " ffer gverin.\n",
      "\n",
      "GICOIOLANIUS:\n",
      "ICIOLAUNouen che bellcesr:\n",
      "Theore chersec: he rom.\n",
      "MENIIILANI freurus,\n",
      "An\n",
      "MEENII whe fr wheem filen.\n",
      "I'd woun, dse beunglivee fure;\n",
      "Marunad where\n",
      "whan.\n",
      "\n",
      "MENIIU S:\n",
      "ANUSI:\n",
      "S\n",
      "\n",
      "\n",
      "iter :4100, loss:62.067446\n",
      "whe vecesus mnoe, tite;angf to pleat It havecorepem ine fere sor ake cthes\n",
      "ower:\n",
      "\n",
      "MEENIOLMNIUS:\n",
      "Whet me by ou frol beart ore qarend esthe ay fou re peros ene:\n",
      "of mpelithe mienagrifes, r menes,\n",
      "Atow,\n",
      "A\n",
      "\n",
      "\n",
      "iter :4200, loss:61.815182\n",
      "d awerand scane sat wab yo thine ome by you pould inen: If oid bere keqe wth yous me whe vee fof farongan ssavin bers we ol you motne y wathe sof comos ffo wead han twhe shave owe have younat, sh ILA:\n",
      "\n",
      "\n",
      "iter :4300, loss:61.958659\n",
      " chat iond ato tim yo' ifod san yowime, omme ias coencke yo wio tloe ven at send:\n",
      "o fagn yo dinolasin willl nowh ar swe sis at pinol ak'de nadins tistin yeq sppple noc;at him If ay Ifo ff com me fit I\n",
      "\n",
      "\n",
      "iter :4400, loss:61.896983\n",
      "Anat soncom mo.\n",
      "\n",
      "Cintr, and Thanke,\n",
      "o by ino nit asinad imwain\n",
      "Add itino to wito fearit san\n",
      "ANUTUS:\n",
      "ICIOLANUS: Harind Youst:\n",
      "Y\n",
      "Mondy oragkinle the ar, is titsen: non t beay yo stou fogr, yuri golas\n",
      "me\n",
      "\n",
      "\n",
      "iter :4500, loss:61.791964\n",
      "yo nithan mmen four thit yo inndister:\n",
      "\n",
      "BRUTUTMartitor im the st porast\n",
      "\n",
      "Fithin fowl,\n",
      "And outh yo mnou terand obillen farurtetion wirist\n",
      "Whillg thist yod,\n",
      "BRUTUThaprenand nt imy handin tanet,\n",
      "\n",
      "ENEENIU\n",
      "\n",
      "\n",
      "iter :4600, loss:61.940602\n",
      "apet dourepreqpeprant omingo hmeny ay Iprerisd gho men tost tithe,\n",
      "Wot irnor?\n",
      "\n",
      "AFirs: itrime tindos, \n",
      "Ciriste pretl's,\n",
      "\n",
      "BRUTUTS:\n",
      "Whiclll' sthot and, ti sreve onaveind timit tiod sih ofrit stinnd ooura\n",
      "\n",
      "\n",
      "iter :4700, loss:62.014441\n",
      "frind\n",
      "Tour this anger vee, saver perosko'd I' mi lspeng towere sthoust\n",
      "restreveerevesexdin gho ouro kurtsond\n",
      "Aun:\n",
      "\n",
      "FDind mepetoro ss:\n",
      "Hal'd I whe sravest\n",
      "Th'ed tere'd swort Thorrs gho shou mis her spr\n",
      "\n",
      "\n",
      "iter :4800, loss:62.044078\n",
      "ame gou banled rou byo hang hhe?\n",
      "As arids grod.\n",
      "\n",
      "CORIUS:\n",
      "Whe's tht rseved hhsive printhe ster sho'd seros fell celellk ay gheu lod bry aver\n",
      "ghar\n",
      "ay say we perotreodu cha.\n",
      "\n",
      "Fimn odl le mand rads\n",
      "Whe po\n",
      "\n",
      "\n",
      "iter :4900, loss:61.950606\n",
      "nu hand engo we hal?\n",
      "\n",
      "ANEI I' the me's masey\n",
      "Th hure, streveist,\n",
      "\n",
      "CORIOL Mase'spervelll he lill beangr\n",
      "Thh han in he'd spervind beereveste ould gian gnuris ll 'tamd teangm avis he mand me anes,\n",
      "\n",
      "Firvi\n",
      "\n",
      "\n",
      "iter :5000, loss:61.866466\n",
      "men, angho?\n",
      "I hus whervis hiesreveewees?\n",
      "ANENEIUS:\n",
      "ICORI I wears shictsho le:\n",
      "Hour ke's polanghir, has shceld wel soud wh?\n",
      "\n",
      "Hoou bellses;\n",
      "Thichlh ht ho ff tlou chn fo'd thepricell rithe had benger she\n",
      "\n",
      "\n",
      "iter :5100, loss:61.823732\n",
      "as;\n",
      "Th hans; fervisel bellas faes welple faol uns har seif focengenat chim tin wil el' wo angm has.\n",
      "\n",
      "CRORIUNIUS:\n",
      "S\n",
      "Cim.\n",
      "\n",
      "CROLNIUS:\n",
      "No bil fou chinouf he man,\n",
      "Wigh oth iong hiv's the pall chis f agm at\n",
      "\n",
      "\n",
      "iter :5200, loss:61.845848\n",
      "it.\n",
      "\n",
      "Aent, bel as thenw hel asrignot hit\n",
      "ei trveibe andigansh thas.\n",
      "He aceings onwas lene.\n",
      "\n",
      "Thh h'at,\n",
      "The thay iso heifo bgo cho ceasi,\n",
      "A stho.\n",
      "\n",
      "DIRUS:\n",
      "Thhell,\n",
      "\n",
      "COROLef iro, 's\n",
      "ANut ow ow.\n",
      "\n",
      "Thy irm?\n",
      "I\n",
      "\n",
      "\n",
      "iter :5300, loss:61.828822\n",
      "ot ano to utrik.\n",
      "\n",
      "CORIOLANUS:\n",
      "Your there.\n",
      "\n",
      "Ast S' tetoelins te ickeid ur gand ogon It ht oou.\n",
      "\n",
      "Sin, I' nate toor wocook eat icou iko couper,\n",
      "\n",
      "Thoch.\n",
      "Se, ave lofea beidla, sol amine.\n",
      "\n",
      "SMINIUS:\n",
      "Yo ntoug\n",
      "\n",
      "\n",
      "iter :5400, loss:61.655921\n",
      "oust ariind.\n",
      "\n",
      "SICROILUNIUS:\n",
      "SI: Ion. Sit,\n",
      "SNIUS:\n",
      "Your,- icotobu tho i ond ou nlin: has then,-- wikon: heth thaic.\n",
      "he tonog ast wirong Ith ibel mea, oncats\n",
      "Sinf haitn poent, ameind ens.\n",
      "\n",
      "SICOf I I and \n",
      "\n",
      "\n",
      "iter :5500, loss:61.724559\n",
      "ot one!\n",
      "isthi ay themay my.\n",
      "\n",
      "Yout I' wod oon goke.\n",
      "\n",
      "Yomut emexes yu iled memay,- Seen:\n",
      "Ydous tonom.\n",
      "Seest toni ent, SI SIf thou the ourt\n",
      "irend, I Sn:\n",
      "Yo ne, ond ont: eney or onte, asthe thhe ter\n",
      "Sipon\n",
      "\n",
      "\n",
      "iter :5600, loss:61.839188\n",
      "imen, onf ed elo orut so butene tyo.\n",
      "os torunkem, at\n",
      "lecodm, was sayur, andfis!\n",
      "\n",
      "The doe mine: heiy ur mape'slls urmau?\n",
      "\n",
      "Yorcesutu mme,\n",
      "Yod moond; nba ben fat Sires felle mey thear we hu apre teens fa\n",
      "\n",
      "\n",
      "iter :5700, loss:62.238236\n",
      "es naterenes yu ppreals fiond, halu, ne veerre nde,\n",
      "Serend Seay!!\n",
      "\n",
      "Yo sty;u thed serve he aure veed oue esrer!\n",
      "\n",
      "Of RIUS:\n",
      "Yourgmas d efer fey!\n",
      "\n",
      "BROLtras sed eray;\n",
      "Ast------\n",
      "SICO I I SI SICNIUS:\n",
      "Heorus \n",
      "\n",
      "\n",
      "iter :5800, loss:62.297988\n",
      "s ur he s boun latl: enor has byoly.\n",
      "fey orey sen ve renve hi lain salk I afre rolus bly desns veasis hpre arend bey oof Reramy Iu lle ff dorernmy, outr I h forr ond helesy fomr ok ende.\n",
      "Sereviersincy\n",
      "\n",
      "\n",
      "iter :5900, loss:62.485663\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 24\n",
    "#read text from the \"input.txt\" file\n",
    "data_generator = DataGenerator('text.txt', sequence_length)\n",
    "gru =  GRU(hidden_size=100, vocab_size=data_generator.vocab_size,sequence_length=sequence_length,learning_rate=0.005)\n",
    "\n",
    "gru.train(data_generator,iterations=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cerus thas that\\nThas bru, May pricsit reitne, band wirs pearvife he wal be lour nusto thos nhely;\\nTht biked erur,\\n\\nTh hh ppont he lalds bun dearve nuf re fay onurary sra lde my, o do dt Sererefrurgans, 'ld\\nThery.\\n\\nSCtir,\\nAy silko, mo bad our I hier nut\\nty\\nORUMINIl Ad sa'l,\\nButur, is ugssarc kouf fe fledr the wror fur sthe myo upresty,\\nHapirt shal; I pre dmel n:\\nYourperins turga, bat hi for ldu hpavi bay urver sory uplaprer fore ne yob urill kellsy uits nwor furs we waod otusr, ray urt tho he goprat lispats teld; wht hai pres rikus!\\n\\nOMaNI In uld waerus, tur bos tol my eful dos anst harad lass the ne by lass 'tr!\\n\\nThthit\\nThals moun kens by ourmpelli\\nAnis bfol the dy rary auppre nfare saky prpery.\\nd cou nd Secourkou pns thapte, d wear pat hipis flais s purave styom.\\n\\nSI for bles wod han fo brald fat enye owlld eve.\\n\\nThes fresal I whil fur haly sior ury wad athor whallve wo uels pury, wikisld naknes serend erbeny\\nA uppraknt he fur woll fe reve\\nrurnd ais bey yorur upr wit elist sally!\\n\\nThim\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru.predict(data_generator, \"c\", 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
